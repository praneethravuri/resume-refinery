This file is a merged representation of the entire codebase, combined into a single document by Repomix.

================================================================
File Summary
================================================================

Purpose:
--------
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.

File Format:
------------
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Multiple file entries, each consisting of:
  a. A separator line (================)
  b. The file path (File: path/to/file)
  c. Another separator line
  d. The full contents of the file
  e. A blank line

Usage Guidelines:
-----------------
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.

Notes:
------
- Some files may have been excluded based on .gitignore rules and Repomix's configuration
- Binary files are not included in this packed representation. Please refer to the Repository Structure section for a complete list of file paths, including binary files
- Files matching patterns in .gitignore are excluded
- Files matching default ignore patterns are excluded

Additional Info:
----------------

================================================================
Directory Structure
================================================================
.gitignore
create_resume_docx.py
data/job_description.txt
data/resume.txt
data/tailored_resume.md
main.py
prompts/system/extract_keywords.md
prompts/system/plan_keywords.md
prompts/system/tailoring.md
prompts/system/update_skills.md
prompts/user/convert_user.md
prompts/user/step1_user.md
prompts/user/step2_user.md
prompts/user/step3_user.md
prompts/user/step4_user.md
requirements.txt

================================================================
Files
================================================================

================
File: .gitignore
================
# Byte-compiled / optimized / DLL files
__pycache__/
*.py[cod]
*$py.class

# C extensions
*.so

# Distribution / packaging
.Python
build/
develop-eggs/
dist/
downloads/
eggs/
.eggs/
lib/
lib64/
parts/
sdist/
var/
wheels/
share/python-wheels/
*.egg-info/
.installed.cfg
*.egg
MANIFEST

# PyInstaller
#  Usually these files are written by a python script from a template
#  before PyInstaller builds the exe, so as to inject date/other infos into it.
*.manifest
*.spec

# Installer logs
pip-log.txt
pip-delete-this-directory.txt

# Unit test / coverage reports
htmlcov/
.tox/
.nox/
.coverage
.coverage.*
.cache
nosetests.xml
coverage.xml
*.cover
*.py,cover
.hypothesis/
.pytest_cache/
cover/

# Translations
*.mo
*.pot

# Django stuff:
*.log
local_settings.py
db.sqlite3
db.sqlite3-journal

# Flask stuff:
instance/
.webassets-cache

# Scrapy stuff:
.scrapy

# Sphinx documentation
docs/_build/

# PyBuilder
.pybuilder/
target/

# Jupyter Notebook
.ipynb_checkpoints

# IPython
profile_default/
ipython_config.py

# pyenv
#   For a library or package, you might want to ignore these files since the code is
#   intended to run in multiple environments; otherwise, check them in:
# .python-version

# pipenv
#   According to pypa/pipenv#598, it is recommended to include Pipfile.lock in version control.
#   However, in case of collaboration, if having platform-specific dependencies or dependencies
#   having no cross-platform support, pipenv may install dependencies that don't work, or not
#   install all needed dependencies.
#Pipfile.lock

# UV
#   Similar to Pipfile.lock, it is generally recommended to include uv.lock in version control.
#   This is especially recommended for binary packages to ensure reproducibility, and is more
#   commonly ignored for libraries.
#uv.lock

# poetry
#   Similar to Pipfile.lock, it is generally recommended to include poetry.lock in version control.
#   This is especially recommended for binary packages to ensure reproducibility, and is more
#   commonly ignored for libraries.
#   https://python-poetry.org/docs/basic-usage/#commit-your-poetrylock-file-to-version-control
#poetry.lock

# pdm
#   Similar to Pipfile.lock, it is generally recommended to include pdm.lock in version control.
#pdm.lock
#   pdm stores project-wide configurations in .pdm.toml, but it is recommended to not include it
#   in version control.
#   https://pdm.fming.dev/latest/usage/project/#working-with-version-control
.pdm.toml
.pdm-python
.pdm-build/

# PEP 582; used by e.g. github.com/David-OConnor/pyflow and github.com/pdm-project/pdm
__pypackages__/

# Celery stuff
celerybeat-schedule
celerybeat.pid

# SageMath parsed files
*.sage.py

# Environments
.env
.venv
env/
venv/
ENV/
env.bak/
venv.bak/

# Spyder project settings
.spyderproject
.spyproject

# Rope project settings
.ropeproject

# mkdocs documentation
/site

# mypy
.mypy_cache/
.dmypy.json
dmypy.json

# Pyre type checker
.pyre/

# pytype static type analyzer
.pytype/

# Cython debug symbols
cython_debug/

# PyCharm
#  JetBrains specific template is maintained in a separate JetBrains.gitignore that can
#  be found at https://github.com/github/gitignore/blob/main/Global/JetBrains.gitignore
#  and can be added to the global gitignore or merged into this file.  For a more nuclear
#  option (not recommended) you can uncomment the following to ignore the entire idea folder.
#.idea/

# Abstra
# Abstra is an AI-powered process automation framework.
# Ignore directories containing user credentials, local state, and settings.
# Learn more at https://abstra.io/docs
.abstra/

# Visual Studio Code
#  Visual Studio Code specific template is maintained in a separate VisualStudioCode.gitignore 
#  that can be found at https://github.com/github/gitignore/blob/main/Global/VisualStudioCode.gitignore
#  and can be added to the global gitignore or merged into this file. However, if you prefer, 
#  you could uncomment the following to ignore the enitre vscode folder
# .vscode/

# Ruff stuff:
.ruff_cache/

# PyPI configuration file
.pypirc

# Cursor
#  Cursor is an AI-powered code editor. `.cursorignore` specifies files/directories to
#  exclude from AI features like autocomplete and code analysis. Recommended for sensitive data
#  refer to https://docs.cursor.com/context/ignore-files
.cursorignore
.cursorindexingignore

.docx

================
File: create_resume_docx.py
================
# create_resume_docx.py

import json
import logging
from docx import Document
from docx.enum.text import WD_TAB_ALIGNMENT, WD_PARAGRAPH_ALIGNMENT
from docx.oxml import OxmlElement
from docx.oxml.ns import qn
from docx.shared import Inches, Pt

logger = logging.getLogger(__name__)


def add_horizontal_line(paragraph):
    """
    Applies a single bottom border to the given paragraph, creating a horizontal line.
    """
    p = paragraph._p
    pPr = p.get_or_add_pPr()
    pBdr = OxmlElement('w:pBdr')
    bottom = OxmlElement('w:bottom')
    bottom.set(qn('w:val'), 'single')
    bottom.set(qn('w:sz'), '6')          # thickness of the line
    bottom.set(qn('w:space'), '1')
    bottom.set(qn('w:color'), '000000')  # black
    pBdr.append(bottom)
    pPr.append(pBdr)
    logger.debug("Added horizontal line to paragraph.")


def make_two_column_paragraph(doc, left_text, right_text, bold_left=False, italic_left=False):
    """
    Creates one paragraph with a right-aligned tab stop.
    The left_text (optionally bold/italic) appears at the left;
    the right_text is pushed to the right margin.
    """
    para = doc.add_paragraph()
    para_format = para.paragraph_format
    para_format.space_before = Pt(0)
    para_format.space_after = Pt(0)
    para_format.line_spacing = 1

    tab_stops = para_format.tab_stops
    tab_stops.add_tab_stop(Inches(7.5), alignment=WD_TAB_ALIGNMENT.RIGHT)

    run_left = para.add_run(left_text)
    if bold_left:
        run_left.bold = True
    if italic_left:
        run_left.italic = True

    para.add_run(f'\t{right_text}')
    logger.debug(f"Created two-column paragraph: '{left_text}' - '{right_text}'.")
    return para


def add_bulleted_item(doc, text):
    """
    Adds a single bullet item with a fixed indent of 0.25".
    """
    para = doc.add_paragraph(style='List Bullet')
    para_format = para.paragraph_format
    para_format.left_indent = Inches(0.25)
    para_format.space_before = Pt(0)
    para_format.space_after = Pt(0)
    para_format.line_spacing = 1

    run = para.add_run(text)
    logger.debug(f"Added bulleted item: '{text[:60]}...'")
    return para


def generate_docx_from_json(resume_data: dict, output_path: str = "Praneeth_Ravuri_Tightened.docx"):
    """
    Given a resume_data dictionary (parsed from JSON), build and save a .docx file
    named output_path. resume_data must have keys: "header", "work_experience",
    "education", "skills", "projects".
    """
    logger.info(f"Generating DOCX at '{output_path}'.")
    doc = Document()

    # -----------------------------
    # 1) Adjust page margins
    # -----------------------------
    section = doc.sections[0]
    section.left_margin = Inches(0.5)
    section.right_margin = Inches(0.5)
    section.top_margin = Inches(0.25)
    section.bottom_margin = Inches(0.25)
    logger.debug("Page margins set: left/right=0.5in, top/bottom=0.25in.")

    # -----------------------------
    # 2) Global default font & paragraph spacing
    # -----------------------------
    style = doc.styles['Normal']
    style.font.name = 'Calibri'
    style.font.size = Pt(10)
    style.paragraph_format.space_before = Pt(0)
    style.paragraph_format.space_after = Pt(0)
    style.paragraph_format.line_spacing = 1
    logger.debug("Global default font set to Calibri 10pt, spacing adjusted.")

    # -----------------------------
    # 3) HEADER (Name + Contact)
    # -----------------------------
    header = resume_data.get("header", {})
    name_para = doc.add_paragraph()
    name_para.alignment = WD_PARAGRAPH_ALIGNMENT.CENTER
    name_para.paragraph_format.space_before = Pt(0)
    name_para.paragraph_format.space_after = Pt(0)
    run = name_para.add_run(header.get("name", ""))
    run.bold = True
    run.font.size = Pt(16)

    contact_para = doc.add_paragraph()
    contact_para.alignment = WD_PARAGRAPH_ALIGNMENT.CENTER
    contact_para.paragraph_format.space_before = Pt(0)
    contact_para.paragraph_format.space_after = Pt(4)
    contact_info = " | ".join(header.get("contact", []))
    run = contact_para.add_run(contact_info)
    run.font.size = Pt(10)
    logger.debug("Header (name + contact) added.")

    # -----------------------------
    # 4) WORK EXPERIENCE
    # -----------------------------
    we_title = doc.add_paragraph()
    we_title.alignment = WD_TAB_ALIGNMENT.LEFT
    we_title.paragraph_format.space_before = Pt(2)
    we_title.paragraph_format.space_after = Pt(0)
    run = we_title.add_run("WORK EXPERIENCE")
    run.bold = True
    run.font.size = Pt(12)
    add_horizontal_line(we_title)

    for exp in resume_data.get("work_experience", []):
        company = exp.get("company", "")
        location = exp.get("location", "")
        position = exp.get("position", "")
        start_date = exp.get("start_date", "")
        end_date = exp.get("end_date", "")

        p1 = make_two_column_paragraph(
            doc,
            left_text=company,
            right_text=location,
            bold_left=True
        )
        p1.paragraph_format.space_before = Pt(4)
        p1.paragraph_format.space_after = Pt(0)

        p2 = make_two_column_paragraph(
            doc,
            left_text=position,
            right_text=f"{start_date} – {end_date}",
            bold_left=True
        )
        p2.paragraph_format.space_before = Pt(0)
        p2.paragraph_format.space_after = Pt(4)

        for bullet in exp.get("bullets", []):
            b = add_bulleted_item(doc, bullet)
            b.paragraph_format.space_before = Pt(0)
            b.paragraph_format.space_after = Pt(0)

    # -----------------------------
    # 5) EDUCATION
    # -----------------------------
    edu_title = doc.add_paragraph()
    edu_title.alignment = WD_TAB_ALIGNMENT.LEFT
    edu_title.paragraph_format.space_before = Pt(4)
    edu_title.paragraph_format.space_after = Pt(0)
    run = edu_title.add_run("EDUCATION")
    run.bold = True
    run.font.size = Pt(12)
    add_horizontal_line(edu_title)

    for edu in resume_data.get("education", []):
        institution = edu.get("institution", "")
        location = edu.get("location", "")
        degree = edu.get("degree", "")
        start_date = edu.get("start_date", "")
        end_date = edu.get("end_date", "")

        p3 = make_two_column_paragraph(
            doc,
            left_text=institution,
            right_text=location,
            bold_left=True
        )
        p3.paragraph_format.space_before = Pt(4)
        p3.paragraph_format.space_after = Pt(0)

        p4 = make_two_column_paragraph(
            doc,
            left_text=degree,
            right_text=f"{start_date} – {end_date}",
        )
        p4.paragraph_format.space_before = Pt(0)
        p4.paragraph_format.space_after = Pt(4)

    # -----------------------------
    # 6) SKILLS
    # -----------------------------
    skills_title = doc.add_paragraph()
    skills_title.alignment = WD_TAB_ALIGNMENT.LEFT
    skills_title.paragraph_format.space_before = Pt(4)
    skills_title.paragraph_format.space_after = Pt(4)
    run = skills_title.add_run("SKILLS")
    run.bold = True
    run.font.size = Pt(12)
    add_horizontal_line(skills_title)

    for skill_group in resume_data.get("skills", []):
        name = skill_group.get("name", "")
        items = skill_group.get("items", [])
        para = doc.add_paragraph()
        para.paragraph_format.space_before = Pt(0)
        para.paragraph_format.space_after = Pt(0)
        para.paragraph_format.line_spacing = 1
        para.add_run(f"{name}: " + ", ".join(items))

    # -----------------------------
    # 7) PROJECTS
    # -----------------------------
    proj_title = doc.add_paragraph()
    proj_title.alignment = WD_TAB_ALIGNMENT.LEFT
    proj_title.paragraph_format.space_before = Pt(4)
    proj_title.paragraph_format.space_after = Pt(0)
    run = proj_title.add_run("PROJECTS")
    run.bold = True
    run.font.size = Pt(12)
    add_horizontal_line(proj_title)

    for proj in resume_data.get("projects", []):
        project_name = proj.get("name", "")
        bullets = proj.get("bullets", [])

        p5 = doc.add_paragraph()
        p5.paragraph_format.space_before = Pt(4)
        p5.paragraph_format.space_after = Pt(4)
        p5.paragraph_format.line_spacing = 1
        run = p5.add_run(project_name)
        run.bold = True

        for bullet in bullets:
            pb = doc.add_paragraph(style='List Bullet')
            pb.paragraph_format.left_indent = Inches(0.25)
            pb.paragraph_format.space_before = Pt(0)
            pb.paragraph_format.space_after = Pt(0)
            pb.paragraph_format.line_spacing = 1
            pb.add_run(bullet)

    # -----------------------------
    # 8) Save the Document
    # -----------------------------
    doc.save(output_path)
    logger.info(f"✅ Document generated: {output_path}")

================
File: data/job_description.txt
================
Are you interested in building innovative technology that shapes the financial markets? Do you like working at the speed of a startup, and tackling some of the world’s most interesting challenges? Do you want to work with, and learn from, hands-on leaders in technology and finance?

At BlackRock, we are looking for Software Engineers who like to innovate and solve sophisticated problems. We recognize that strength comes from diversity, and will embrace your outstanding skills, curiosity, and passion while giving you the opportunity to grow technically and as an individual.

We invest and protect over $9 trillion (USD) of assets and have an extraordinary responsibility to our clients all over the world. Our technology empowers millions of investors to save for retirement, pay for college, buy a home and improve their financial well-being.

Being a technologist at BlackRock means you get the best of both worlds: working for one of the most sophisticated financial companies and being part of a software development team responsible for next generation technology and solutions.

Team Overview:
The Aladdin Product Group’s Post Trade & Accounting Team provides technology solutions for BlackRock's post trade investment life cycle operational processes including Accounting & Reporting platform to support the scale at which BlackRock operates. The team is responsible for a suite of applications including highly complex and scalable performance calculation engines, investment accounting, client reporting systems, platform for reconciliation, derivative management. We innovate incrementally and focus on high level of client service through responsiveness, accuracy, and efficiency.

Responsibilities:

Design, develop, deliver, and maintain web applications with a focus on both frontend and backend components.
Demonstrate technical leadership of software design & architecture to support strategic product roadmap,
Deliver plans and define milestones, prioritize initiatives, and properly allocate resources to meet project delivery and business commitments
Collaborate with cross-functional teams in a multi-office, multi-country environment to define, design, and ship high-quality software solutions.
Implement responsive user interface apps using Angular and ensure seamless integration with backend services with an API-first approach.
Design and develop innovative solutions to complex problems, identifying issues and roadblocks.
Work with project managers, technical leads, and business analysts to contribute throughout the SDLC cycle.
Manage stakeholders for driving business decisions, negotiating priorities, and partner with various business teams to drive strategy and technology adoption
Ensure scale, resilience and stability through risk identification and mitigation, quality code reviews, creating robust test suites, and providing level two support.
Foster continuous improvement in software development practices through innovation and automation.
Stay up to date with industry trends and emerging technologies to continuously improve development processes.
Skills & Experience:

Experience leading development teams or projects of a significant application, system, or component.
Hands-on programming experience in Java, Python, JavaScript, Typescript with OO skills and design patterns
Experience with Open Source technology stacks and frameworks (Spring, Angular, React, Tomcat, Maven, Junit, NodeJS, Express)
Experience working with relational and NoSQL databases (such as SQL Server, Apache Cassandra)
Strong problem-solving, analytical, and software architecture skills.
Experience in partnering with other teams, sponsors, and user groups who are on the same product journey.
Ability to work in Agile/Scrum development environments with strong teamwork, communication, and time management skills.
Innovative and a thought leader around new/cutting-edge technologies.
Nice to Have:

Experience with cloud native tools (such as Kubernetes, Docker) and cloud platforms (such as Azure, AWS, or GCP)
Experience with frontend frameworks & responsive web design (WebPack, HTML5, CSS3) is a plus.
Exposure to high scale distributed technologies such as Kafka, Ignite, Redis.
Experience with DevOps, continuous integration and continuous deployment (CI/CD) pipelines, and tools like Azure DevOps.
Experience or real interest in finance, investment processes, and/or an ability to translate business problems into technical solutions.

================
File: data/resume.txt
================
WORK EXPERIENCE
Prodapt North America Richardson, TX
Software Engineer October 2024 – March 2025
● Architected ETL pipelines using Go and Kafka to process 5,000+ network device metrics and enable real-time performance monitoring,
streamline issue resolution, and improve network stability.
● Optimized database queries and implemented batch processing to ingest 1 million+ JSON objects per hour into GCP BigQuery and MongoDB.
● Configured custom decoders for 170+ network device metrics, resolving persistent data ingestion issues and improving data reliability.
● Designed a real-time monitoring system using Python that decreased incident detection time by 40% and automated recovery procedures.
● Established CI/CD automation with GitHub Actions and Docker containerization, reducing deployment time by 60% and increasing test
coverage to 85%.
Cognizant Hyderabad, India
Full Stack Software Engineer Intern January 2022 – June 2022
● Designed a real-time messaging system serving 500+ employees using React, Node.js, and Express.js with secure authentication.
● Enhanced API performance by implementing asynchronous operations and Redis caching, increasing throughput from 200 to 1,000 RPS and
reducing response time from 150ms to 50ms.
● Boosted notification speed and cut cloud cost overhead by 30% by optimizing AWS Lambda using cold-start reduction.
● Deployed authentication, notification, and traffic management microservices with Kubernetes, achieving zero-downtime deployments and
automatic scaling.
● Automated AWS provisioning via Terraform, slashing setup time from 1 hour to 15 minutes.
● Engineered PostgreSQL optimization techniques including strategic indexing and table partitioning, improving query response times by 65%
and supporting 3x higher concurrent user loads.
EDUCATION
George Mason University Fairfax, VA
Master of Science, Computer Science August 2022 – May 2024
Coursework: Data Structures, Algorithms, Operating Systems, Computer Networks, Database Management Systems, Web Development
Gokaraju Rangaraju Institute of Engineering and Technology Hyderabad, India
Bachelor of Technology, Computer Science June 2018 – May 2022
Coursework: Cybersecurity, Software Testing, Cloud Computing, Object-Oriented Programming, Operating Systems
SKILLS
Programming Languages: Python, Go, JavaScript, TypeScript
Web Technologies: React, Next.js, Node.js, Express.js, FastAPI, Flask
Databases: PostgreSQL, MySQL, MongoDB, Redis, Pinecone
Cloud, DevOps, and Version Control: Amazon Web Services Lambda, EC2, Docker, Kubernetes, Terraform, Google Cloud Platform BigQuery, Kafka,
CI/CD, GitHub Actions, Git, GitHub
Methodologies: Agile/Scrum, Test-Driven Development (TDD), SDLC, Object-Oriented Programming
PROJECTS
RAG Document Assistant
● Built a full-stack RAG system with FastAPI backend and Next.js frontend that generates contextually relevant responses from document
libraries.
● Optimized vector search by implementing embedding preprocessing and duplicate detection, reducing retrieval times by 40%.
● Incorporated document preprocessing pipeline with text normalization, chunking, and metadata extraction to improve context relevance.
● Created UI components like LLM model selector, chat features, and document upload using Next.js, React, and Tailwind CSS.
Smart Traffic Management System
● Developed a reinforcement learning model using Deep Q-Networks (DQN) with TensorFlow to optimize traffic signal timing based on
simulated traffic conditions.
● Formulated a custom reward function using NumPy arrays for mathematical operations, balancing vehicle wait time and throughput,
fine-tuning the model across 1000+ generations.
● Constructed a simulation environment with Python and Pygame to validate the approach across various traffic patterns.
● Leveraged scikit-learn for data preprocessing and feature engineering, enabling the model to process traffic patterns more efficiently.

================
File: data/tailored_resume.md
================
```markdown
## WORK EXPERIENCE

### Prodapt North America Richardson, TX
Software Engineer October 2024 – March 2025
- Architected ETL pipelines using Java and Kafka to process 5,000+ network metrics, enhancing real-time performance monitoring.
- Optimized database queries and batch processing to ingest 1 million+ JSON objects per hour into GCP BigQuery and Cassandra.
- Established CI/CD automation with GitHub Actions and Maven, reducing deployment time by 60% and increasing test coverage to 85%.
- Led a team to design a monitoring system, decreasing incident detection time by 40% and automating recovery procedures.
- Communicated effectively with stakeholders to resolve data ingestion issues, improving data reliability and project delivery.

### Cognizant Hyderabad, India
Full Stack Software Engineer Intern January 2022 – June 2022
- Designed a real-time messaging system for 500+ employees using React, Node.js, and Spring with secure authentication.
- Enhanced API performance with asynchronous operations and Junit testing, increasing throughput from 200 to 1,000 RPS.
- Innovated AWS Lambda optimization, reducing cloud costs by 30% and improving notification speed.
- Deployed microservices with Kubernetes and Tomcat, achieving zero-downtime deployments and automatic scaling.
- Collaborated cross-functionally to implement SQL Server optimizations, improving query response times by 65%.

## EDUCATION

### George Mason University Fairfax, VA
Master of Science, Computer Science August 2022 – May 2024  
Coursework: Data Structures, Algorithms, Operating Systems, Computer Networks, Database Management Systems, Web Development

### Gokaraju Rangaraju Institute of Engineering and Technology Hyderabad, India
Bachelor of Technology, Computer Science June 2018 – May 2022  
Coursework: Cybersecurity, Software Testing, Cloud Computing, Object-Oriented Programming, Operating Systems

## SKILLS

- **Programming Languages:** Python, Go, JavaScript, TypeScript, Java
- **Web Technologies:** React, Next.js, Node.js, Express.js, FastAPI, Flask, Angular, Spring
- **Databases:** PostgreSQL, MySQL, MongoDB, Redis, Pinecone, SQL Server, Apache Cassandra
- **Cloud, DevOps, and Version Control:** Amazon Web Services Lambda, EC2, Docker, Kubernetes, Terraform, Google Cloud Platform BigQuery, Kafka, CI/CD, GitHub Actions, Git, GitHub, Maven
- **ML / AI:** TensorFlow, NumPy, scikit-learn
- **Methodologies:** Agile/Scrum, Test-Driven Development (TDD), SDLC, Object-Oriented Programming

## PROJECTS

### RAG Document Assistant
- Built a full-stack RAG system with FastAPI backend and Angular frontend, generating contextually relevant responses from document libraries.
- Enhanced responsive design practices, improving user experience and accessibility across multiple devices.
- Incorporated document preprocessing pipeline with text normalization, chunking, and metadata extraction to improve context relevance.
- Analyzed data to optimize vector search, reducing retrieval times by 40% through embedding preprocessing and duplicate detection.

### Smart Traffic Management System
- Developed a reinforcement learning model using Deep Q-Networks (DQN) with TensorFlow to optimize traffic signal timing.
- Formulated a custom reward function using NumPy for mathematical operations, balancing vehicle wait time and throughput.
- Constructed a simulation environment with Python and Pygame to validate the approach across various traffic patterns.
- Leveraged scikit-learn for data preprocessing, enabling efficient processing of traffic patterns.
```

================
File: main.py
================
# main.py

import logging
from logging.handlers import RotatingFileHandler
import math
import json
import os
from pathlib import Path
from openai import OpenAI
from dotenv import load_dotenv

from create_resume_docx import generate_docx_from_json

# --------------------------------------------------
# 1) Logging Configuration: write to logs/resume_tailoring.log
# --------------------------------------------------
LOG_DIR = Path("logs")
LOG_DIR.mkdir(exist_ok=True)
log_path = LOG_DIR / "resume_tailoring.log"

file_handler = RotatingFileHandler(
    filename=log_path,
    maxBytes=5 * 1024 * 1024,  # 5 MB
    backupCount=3
)
file_handler.setLevel(logging.DEBUG)
formatter = logging.Formatter(
    "%(asctime)s %(levelname)-8s [%(name)s:%(lineno)d] %(message)s",
    datefmt="%Y-%m-%d %H:%M:%S"
)
file_handler.setFormatter(formatter)

root_logger = logging.getLogger()
root_logger.setLevel(logging.DEBUG)
root_logger.addHandler(file_handler)

# Also print WARNING+ to console
console_handler = logging.StreamHandler()
console_handler.setLevel(logging.WARNING)
console_handler.setFormatter(formatter)
root_logger.addHandler(console_handler)

logger = logging.getLogger(__name__)

# --------------------------------------------------
# 2) Configuration of folder paths
# --------------------------------------------------
ROOT_DIR = Path(__file__).parent
DATA_DIR = ROOT_DIR / "data"
PROMPTS_DIR = ROOT_DIR / "prompts"
TEMP_DIR = ROOT_DIR / "temp"
RESUMES_DIR = ROOT_DIR / "resumes"

JD_PATH = DATA_DIR / "job_description.txt"
RESUME_PATH = DATA_DIR / "resume.txt"
TAILORED_RESUME_PATH = DATA_DIR / "tailored_resume.md"
REPORT_PATH = TEMP_DIR / "report.md"

# Prompt files:
STEP1_SYSTEM_PROMPT = PROMPTS_DIR / "system" / "extract_keywords.md"
STEP2_SYSTEM_PROMPT = PROMPTS_DIR / "system" / "plan_keywords.md"
TAILORING_PROMPT = PROMPTS_DIR / "system" / "tailoring.md"
STEP4_SYSTEM_PROMPT = PROMPTS_DIR / "system" / "update_skills.md"

STEP1_USER_PROMPT = PROMPTS_DIR / "user" / "step1_user.md"
STEP2_USER_PROMPT = PROMPTS_DIR / "user" / "step2_user.md"
STEP3_USER_PROMPT = PROMPTS_DIR / "user" / "step3_user.md"
STEP4_USER_PROMPT = PROMPTS_DIR / "user" / "step4_user.md"
CONVERT_USER_PROMPT = PROMPTS_DIR / "user" / "convert_user.md"

# --------------------------------------------------
# 3) Helper: call OpenAI with logging and retries
# --------------------------------------------------
def call_openai_with_logging(messages, purpose, model="gpt-4o", temperature=0.3, max_tokens=4000):
    """
    Wraps an OpenAI chat call, logs inputs/outputs, and returns the assistant's content.
    """
    logger.debug(f"[{purpose}] Sending {len(messages)} messages to OpenAI (model={model}, temp={temperature}).")
    try:
        response = client.chat.completions.create(
            model=model,
            messages=messages,
            temperature=temperature,
            max_tokens=max_tokens,
        )
        result = response.choices[0].message.content.strip()
        logger.debug(f"[{purpose}] Received response:\n{result[:200]}{'...' if len(result) > 200 else ''}")
        return result
    except Exception as e:
        logger.error(f"[{purpose}] Error calling OpenAI: {e}", exc_info=True)
        raise

# --------------------------------------------------
# 4) Helper: read text from a file (or raise)
# --------------------------------------------------
def read_text_file(path: Path) -> str:
    if not path.exists():
        logger.error(f"Missing required file: {path}")
        raise FileNotFoundError(f"File not found: {path}")
    text = path.read_text(encoding="utf-8")
    logger.debug(f"Read {len(text)} characters from {path}.")
    return text

# --------------------------------------------------
# 5) Helper: compute cosine similarity of two embeddings
# --------------------------------------------------
def cosine_similarity(vec1, vec2):
    """
    Compute cosine similarity between two vectors.
    """
    dot = sum(a * b for a, b in zip(vec1, vec2))
    mag1 = math.sqrt(sum(a * a for a in vec1))
    mag2 = math.sqrt(sum(b * b for b in vec2))
    if mag1 == 0 or mag2 == 0:
        return 0.0
    return dot / (mag1 * mag2)

# --------------------------------------------------
# 6) Main pipeline
# --------------------------------------------------
def run_pipeline(company: str, position: str, jobid: str):
    # Ensure necessary directories exist
    TEMP_DIR.mkdir(exist_ok=True)
    DATA_DIR.mkdir(exist_ok=True)
    RESUMES_DIR.mkdir(exist_ok=True)

    # --- STEP 1: EXTRACT & RANK KEYWORDS ---
    job_description = read_text_file(JD_PATH)
    resume_text = read_text_file(RESUME_PATH)

    step1_system = read_text_file(STEP1_SYSTEM_PROMPT)
    step1_user_tpl = read_text_file(STEP1_USER_PROMPT)
    step1_user = step1_user_tpl.replace("<JOB_DESCRIPTION>", job_description)

    ranked_keywords = call_openai_with_logging(
        [
            {"role": "system", "content": step1_system},
            {"role": "user", "content": step1_user}
        ],
        purpose="step1_extract_keywords"
    )
    logger.info("Step 1 completed: ranked keywords obtained.")

    # --- STEP 2: PLAN KEYWORD INSERTION ---
    step2_system = read_text_file(STEP2_SYSTEM_PROMPT)
    step2_user_tpl = read_text_file(STEP2_USER_PROMPT)
    step2_user = (
        step2_user_tpl
        .replace("<RESUME>", resume_text)
        .replace("<RANKED_KEYWORDS>", ranked_keywords)
    )

    tailoring_plan = call_openai_with_logging(
        [
            {"role": "system", "content": step2_system},
            {"role": "user", "content": step2_user}
        ],
        purpose="step2_plan_keywords"
    )
    logger.info("Step 2 completed: tailoring plan obtained.")

    # --- STEP 3: APPLY TAILORING PLAN (intermediate, not written to disk) ---
    step3_user_tpl = read_text_file(STEP3_USER_PROMPT)
    step3_user = (
        step3_user_tpl
        .replace("<RESUME>", resume_text)
        .replace("<TAILORING_PLAN>", tailoring_plan)
    )

    tailored_resume_mid = call_openai_with_logging(
        [
            {"role": "system", "content": read_text_file(TAILORING_PROMPT)},
            {"role": "user", "content": step3_user}
        ],
        purpose="step3_apply_tailoring"
    )
    logger.info("Step 3 completed: intermediate tailored resume obtained.")

    # --- STEP 4: UPDATE SKILLS SECTION ---
    step4_system = read_text_file(STEP4_SYSTEM_PROMPT)
    step4_user = read_text_file(STEP4_USER_PROMPT).replace("<TAILORED_RESUME>", tailored_resume_mid)

    final_resume = call_openai_with_logging(
        [
            {"role": "system", "content": step4_system},
            {"role": "user", "content": step4_user}
        ],
        purpose="step4_update_skills"
    )
    # Write the final tailored resume (with updated skills) to data/tailored_resume.md
    TAILORED_RESUME_PATH.write_text(final_resume, encoding="utf-8")
    logger.info(f"Step 4 completed: final tailored resume written to {TAILORED_RESUME_PATH}.")

    # --- CONSOLIDATE INTO A SINGLE REPORT.MD ---
    report_lines = []
    report_lines.append("# Step 1: Ranked Keywords\n")
    report_lines.append(ranked_keywords + "\n")
    report_lines.append("# Step 2: Tailoring Plan\n")
    report_lines.append(tailoring_plan + "\n")
    report_lines.append("# Step 4: Final Tailored Resume\n")
    report_lines.append(final_resume + "\n")

    REPORT_PATH.write_text("".join(report_lines), encoding="utf-8")
    logger.info(f"Consolidated report written to {REPORT_PATH}.")

    # --- STEP 5: EMBEDDING-BASED SIMILARITY SCORE ---
    logger.info("Computing embedding for Job Description...")
    jd_embed = client.embeddings.create(model="text-embedding-ada-002", input=job_description).data[0].embedding

    logger.info("Computing embedding for Final Tailored Resume...")
    resume_embed = client.embeddings.create(model="text-embedding-ada-002", input=final_resume).data[0].embedding

    similarity_score = cosine_similarity(jd_embed, resume_embed)
    similarity_pct = round(similarity_score * 100, 2)
    sim_output = f"Cosine similarity (JD ↔ Tailored Resume): {similarity_score:.4f} (~{similarity_pct}%)\n"
    Path(TEMP_DIR / "similarity_score.md").write_text(sim_output, encoding="utf-8")
    logger.info("Similarity score saved to temp/similarity_score.md.")

    # --- STEP 6: CONVERT FINAL TAILORED RESUME TO JSON ---
    convert_user_tpl = read_text_file(CONVERT_USER_PROMPT)
    convert_user = convert_user_tpl.replace("<FINAL_TAILORED_RESUME>", final_resume)

    resume_body_json_text = call_openai_with_logging(
        [
            {"role": "system", "content": "You are a strict JSON‐only assistant. Output exactly one JSON blob."},
            {"role": "user", "content": convert_user}
        ],
        purpose="step6_convert_to_json"
    )

    # Cleanup raw model output of backticks or quotes
    cleaned = resume_body_json_text.strip()
    if cleaned.startswith("```"):
        lines = cleaned.splitlines()
        if lines[0].startswith("```"):
            lines = lines[1:]
        if lines and lines[-1].startswith("```"):
            lines = lines[:-1]
        cleaned = "\n".join(lines).strip()
    if cleaned.startswith('"""') and cleaned.endswith('"""'):
        cleaned = cleaned[3:-3].strip()
    if cleaned.startswith('"') and cleaned.endswith('"'):
        cleaned = cleaned[1:-1]

    try:
        resume_body = json.loads(cleaned)
    except json.JSONDecodeError as e:
        logger.error("Failed to parse JSON from model output for resume conversion: %s\nRaw output:\n%s", e, resume_body_json_text)
        raise

    # Attach the constant header
    HEADER_DATA = {
        "name": "Praneeth Ravuri",
        "contact": [
            "Frisco, TX",
            "(571) 622-8648",
            "pravdevrav@gmail.com",
            "github.com/praneethravuri",
            "praneethravuri.com",
            "linkedin.com/in/prav25/"
        ]
    }
    full_resume_json = {
        "header": HEADER_DATA,
        **resume_body
    }
    Path(TEMP_DIR / "final_resume.json").write_text(json.dumps(full_resume_json, indent=2), encoding="utf-8")
    logger.info("Step 6 completed: final_resume.json written to temp/final_resume.json.")

    # --- STEP 7: GENERATE DOCX COPIES ---
    def _sanitize(text: str) -> str:
        return "_".join(text.strip().split()).replace("/", "_")

    company_clean = _sanitize(company)
    position_clean = _sanitize(position)
    jobid_clean = _sanitize(jobid) if jobid.strip() else ""

    if jobid_clean:
        filename1 = f"{company_clean}_{position_clean}_{jobid_clean}.docx"
    else:
        filename1 = f"{company_clean}_{position_clean}.docx"

    downloads_dir = os.path.join(os.path.expanduser("~"), "Downloads")
    if not os.path.isdir(downloads_dir):
        os.makedirs(downloads_dir, exist_ok=True)
    filename2 = "praneeth_ravuri_resume.docx"
    download_path = os.path.join(downloads_dir, filename2)

    with open(TEMP_DIR / "final_resume.json", "r", encoding="utf-8") as f:
        resume_json_data = json.load(f)

    # 1st copy: named by company_position_jobid, saved under resumes/
    generate_docx_from_json(resume_json_data, output_path=f"resumes/{filename1}")
    logger.info(f"Generated DOCX: resumes/{filename1}")

    # 2nd copy: stored in Downloads folder under fixed name
    generate_docx_from_json(resume_json_data, output_path=download_path)
    logger.info(f"Generated DOCX copy in Downloads: {download_path}")

    print("✅ All steps completed. See logs in 'logs/resume_tailoring.log' for details.")


if __name__ == "__main__":
    load_dotenv()
    client = OpenAI()

    print("Please enter the following details for naming the resume files.")
    company = input("Company Name: ").strip()
    position = input("Position Name: ").strip()
    jobid = input("Job ID (optional; press Enter to skip): ").strip()

    try:
        run_pipeline(company, position, jobid)
    except Exception as e:
        logger.error("Pipeline terminated with an error: %s", e, exc_info=True)
        print(f"❌ Pipeline failed: {e}")

================
File: prompts/system/extract_keywords.md
================
You are a keyword extraction assistant.

Extract all tools, technologies, keywords, and any soft skills mentioned
in the following job description. Rank them by relevance to a full-stack/
software engineer role. Additionally, note any terms or phrases that appear
multiple times or seem emphasized (e.g., repeated skills, core competencies,
or culture hints). List each unique term once. Do not invent any items
not present in the text.

--------------------------
<JOB_DESCRIPTION_PLACEHOLDER>
--------------------------

================
File: prompts/system/plan_keywords.md
================
You are a planning assistant who maps extracted keywords into a resume.

For each keyword in the job description, indicate:
  • Whether to **Replace** an existing bullet (if it does not appear verbatim in any bullet), or **Skip** it (only if it already appears in a bullet exactly as-is).
  • If “Replace,” specify:
      – Section (e.g., “Prodapt (October 2024–March 2025)” or “RAG Document Assistant (Project)”)
      – A one-line snippet (from the original resume) of the bullet to be overwritten
      – A one-sentence justification for replacing that bullet, mentioning any repeated/emphasized JD terms or culture hints.
  • If a keyword already appears in a bullet exactly, mark “Skip.” Do not allow “Skip” if the keyword is missing entirely from bullets.

Format your output as a Markdown table with columns:

| Keyword    | Action  | Section                             | Line Snippet to Replace                                                                                                              | Justification                                                                                                |
| ---------- | ------- | ----------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------ | ------------------------------------------------------------------------------------------------------------ |
| Kubernetes | Replace | Cognizant (January 2022–June 2022) | “Deployed microservices with Docker”                                                                                               | “JD stresses container orchestration; overwrite this bullet to include Kubernetes instead of Docker.”      |
| React      | Replace | Prodapt (October 2024–March 2025)  | “Architected ETL pipelines using Go and Kafka”                                                                                     | “JD emphasizes front-end React experience; replace this backend-only bullet with one that mentions React.” |
| Senior     | Replace | Prodapt (Job Title)                 | “Software Engineer”                                                                                                                | “JD requires ‘Senior Software Engineer’; replace the title exactly to match JD language.”                |
| AWS Lambda | Replace | Cognizant (January 2022–June 2022) | “Boosted notification speed and cut cloud cost overhead by 30% by optimizing AWS Lambda.”                                          | “JD highlights serverless architectures; rewrite to specify event-driven Lambda patterns.”                 |
| PostgreSQL | Skip    | —                                  | —                                                                                                                                   | “Already present in bullets; no change.”                                                                   |
| Terraform  | Replace | Prodapt (October 2024–March 2025)  | “Established CI/CD automation with GitHub Actions and Docker.”                                                                     | “JD lists Terraform for IaC; overwrite to mention Terraform usage instead of just Docker.”                 |
| TDD        | Replace | Cognizant (January 2022–June 2022) | “Enhanced API performance by implementing asynchronous operations and Redis caching, increasing throughput from 200 to 1,000 RPS.” | “JD highlights test-driven development; rewrite to include TDD practices.”                                 |
| FastAPI    | Replace | RAG Document Assistant (Project)    | “Built a full-stack RAG system with FastAPI backend and Next.js frontend.”                                                         | “JD requires modern Python frameworks; refine this bullet to emphasize asynchronous FastAPI endpoints.”    |
| …         | …      | …                                  | …                                                                                                                                   | …                                                                                                           |

================
File: prompts/system/tailoring.md
================
YOU ARE AN ELITE RESUME TAILORING AGENT FOR SOFTWARE ENGINEER AND FULL STACK ENGINEER ROLES.
YOUR PURPOSE IS TO CREATE A LASER-ALIGNED, ATS-OPTIMIZED RESUME THAT STRATEGICALLY INTEGRATES JOB
DESCRIPTION KEYWORDS INTO RELEVANT SECTIONS OF THE RESUME — PRIORITIZING IMPACT, DOMAIN ACCURACY,
AND REALISM.

---

### TAILORING OBJECTIVES

YOU MUST:

- ALIGN THE RESUME TO SOFTWARE ENGINEER & FULL STACK ENGINEER ROLES ONLY.
- USE ONLY STRONG, SIMPLE, MEASURABLE METRICS (numbers, percentages, time saved, performance gained).
- AVOID VAGUE METRICS (e.g., “reduced manual effort” must become a measurable result).
- PRESERVE DOMAIN RELEVANCE — ENSURE RESUME BULLETS MATCH THE INDUSTRY/DOMAIN OF THE JOB.
- ONLY MODIFY EXISTING EXPERIENCES AND PROJECTS — DO NOT CREATE NEW WORK EXPERIENCES OR PROJECTS.
- LIMIT TO:
  • 5 BULLET POINTS PER WORK EXPERIENCE• 4 BULLET POINTS PER PROJECT
- EACH BULLET MUST FIT IN ONE LINE ONLY.
- EACH BULLET MUST CONTAIN 15–18 WORDS. If a bullet exceeds these limits, shorten or lengthen it without losing core meaning, metric, keywords, and technologies.
- MAINTAIN SHARP, CONCISE LANGUAGE WITH CLEAR ACTION VERBS AND TECHNICAL FOCUS.
- NEVER USE VAGUE PHRASES like “streamlined workflow,” “reduced manual effort” without measurable detail.
- EACH BULLET POINT SHOULD START WITH A UNIQUE STRONG ACTION VERB.
- MATCH YOUR PHRASES AND TERMINOLOGY TO THE LANGUAGE USED IN THE JOB DESCRIPTION (e.g., use the same skill names and verbs the JD uses).
- PAY ATTENTION TO COMPANY-CULTURE HINTS (e.g., “collaborative,” “fast-paced startup,” “innovative environment”) and adapt tone or phrasing to reflect that culture where possible.
- IF THE JOB DESCRIPTION SPECIFIES A POSITION TITLE FOR “Prodapt North America,” REPLACE THE EXISTING ‘Software Engineer’ TITLE IN THAT ENTRY WITH THE JD TITLE EXACTLY (NO PARENTHESES). DO NOT MODIFY ANY OTHER COMPANY TITLES.
- NEVER REPEAT THE SAME TOOL ACROSS MULTIPLE BULLETS.
- **EVERY TECHNOLOGY OR TOOL FROM THE JOB DESCRIPTION MUST BE INTEGRATED INTO AT LEAST ONE EXISTING BULLET. YOU MAY NOT LEAVE A KEYWORD OUT. IF A KEYWORD IS MISSING FROM ALL BULLETS, YOU MUST OVERWRITE A LESS-RELEVANT BULLET WITH A NEW ONE THAT INCLUDES THE KEYWORD (15–18 WORDS, METRIC, DOMAIN CONTEXT).**
- NEVER INSERT KEYWORDS ARTIFICIALLY WITHOUT DOMAIN RELEVANCE.
- NEVER INCLUDE TOOLS IN SKILLS THAT ARE NOT MENTIONED IN AN EXISTING BULLET OR PROJECT.
- NEVER KEEP POINTS THAT ARE IRRELEVANT TO THE TARGET DOMAIN.
- NEVER EXCEED LINE LENGTH OR BULLET COUNT LIMITS PER SECTION.
- NEVER OUTPUT EMOJI, SPECIAL CHARACTERS, OR MARKETING-LIKE SENTENCES.
- NEVER CREATE NEW WORK EXPERIENCES OR PROJECTS THAT DO NOT EXIST.
- NEVER REMOVE ANY EXISTING SKILLS FROM THE SKILLS SECTION.

---

### TECHNOLOGY PRIORITIZATION DIRECTIVE

FOR EACH TECHNOLOGY OR SKILL IN THE JOB DESCRIPTION:

1. **REPLACE** a current bullet to include that keyword.
   - If the keyword does not appear naturally in any bullet, you must overwrite one less-relevant bullet with a new version that includes the keyword, preserving:
     • Unique strong action verb
     • One measurable metric
     • Proper domain context
     • Exactly 15–18 words
2. DO NOT repeat a keyword more than once across bullets.
3. ONLY AFTER A KEYWORD APPEARS IN A BULLET MAY YOU REFLECT IT IN THE SKILLS SECTION.

---

### SKILLS SECTION INTEGRATION

- EVERY TECHNOLOGY INCLUDED IN AT LEAST ONE BULLET MUST APPEAR IN THE CORRECT SKILLS CATEGORY.
- DO NOT REMOVE ANY EXISTING SKILLS FROM THE SKILLS SECTION.
- IF A KEYWORD IS NOT USED IN ANY BULLET, **DO NOT** LIST IT IN SKILLS.

---

### BULLET-POINT BEST PRACTICES

Below are few-shot examples illustrating good vs. bad bullet points:

- **BAD BULLET** (VAGUE, NO METRIC, NO ACTION VERB):Worked on data pipelines to help analytics team.
- **GOOD BULLET** (STRONG VERB, METRIC, TECHNOLOGY):Built data pipelines with Apache Airflow, reducing ETL time by 50% and enabling daily insights generation.
- **TOP BIG TECH EXAMPLE** (for reference):Optimized RESTful API endpoints in Node.js, increasing throughput by 200% and cutting latency to under 50 ms.
- **ANOTHER TOP EXAMPLE**:
  Engineered horizontally scalable microservices using Kubernetes on AWS, improving uptime to 99.99% and reducing costs by 30%.

---

### CHAIN OF THOUGHTS TO FOLLOW

1. **UNDERSTAND:** Scan the Job Description for all relevant tools, technologies, frameworks, repeated/emphasized terms, and any culture hints.
2. **MAP:** For each extracted keyword, find where in the current bullets it could be meaningfully integrated.
3. **REPLACE:** If the keyword does not already appear in a bullet verbatim, overwrite exactly one existing bullet (preferably the least domain-relevant) with a new bullet that:
   - Starts with a unique strong action verb
   - Contains exactly one measurable metric
   - Mentions the keyword in context
   - Totals 15–18 words
4. **VERIFY:** Once a keyword appears in a bullet, add it to the correct category in Skills (without removing any existing skills).
5. **ADJUST TITLES FOR PRODAPT ONLY:** If JD specifies a new title, change “Software Engineer” to that title (no parentheses).
6. **ADAPT TONE:** If JD hints at culture (e.g., “fast-paced,” “innovative”), tweak verbs/adjectives accordingly.

---

### BANNED PHRASES & JARGON TO AVOID

**JARGON & BUZZWORDS TO AVOID:**
“Synergized”, “ideated”, “disruptive”, “next-gen”, “leveraged”, “ecosystem”, “paradigm”

**FILLER WORDS TO REMOVE:**
“Responsible for”, “helped with”, “participated in”

---

### WHAT NOT TO DO

- NEVER repeat the same tool across multiple bullets.
- NEVER insert keywords without domain relevance.
- NEVER include tools in Skills that are not mentioned in an existing bullet.
- NEVER keep points irrelevant to the target domain.
- NEVER exceed line length or bullet count limits per section.
- NEVER output emoji, special characters, or marketing-like sentences.
- NEVER create new work experiences or projects that do not exist.
- NEVER remove any existing skills from the Skills section.

================
File: prompts/system/update_skills.md
================
You are a resume skills‐section expert.

Given the final, tailored resume (with all bullets already modified), update its “Skills” section so that:
  1. Every technology, library, or framework used in any bullet is present in the appropriate category.
  2. No existing skill is removed.
  3. Group them logically into categories such as:
        • Programming Languages
        • Web Technologies
        • Databases
        • Cloud / DevOps
        • ML / AI
        • Methodologies
  4. Maintain existing formatting for the other sections; output the full resume text with only the Skills section modified.

--------------------------
<TAILORED_RESUME_PLACEHOLDER>
--------------------------

================
File: prompts/user/convert_user.md
================
You are a resume-to-JSON converter. Take the following full resume text (excluding header) and output exactly one JSON object with these keys:
  - "work_experience": list of objects
  - "education": list of objects
  - "skills": list of skill-group objects
  - "projects": list of project objects

Each object should match this structure:

work_experience entry:
{
  "company": "<Company Name>",
  "location": "<City, State>",
  "position": "<Position Title>",
  "start_date": "<Month Year>",
  "end_date": "<Month Year or Present>",
  "bullets": ["<Bullet 1>", "<Bullet 2>", "... up to 5 bullets"]
}

education entry:
{
  "institution": "<Institution Name>",
  "location": "<City, State or Country>",
  "degree": "<Degree and Major>",
  "start_date": "<Month Year>",
  "end_date": "<Month Year>"
}

skill-group entry:
{
  "name": "<Category Name>",
  "items": ["<Item 1>", "<Item 2>", "..."]
}

project entry:
{
  "name": "<Project Name>",
  "bullets": ["<Bullet 1>", "<Bullet 2>", "... up to 4 bullets"]
}

**IMPORTANT**:
- Output exactly one JSON blob—no extra text, no markdown fences, no comments.
- The JSON must start with '{' and end with '}' and be valid.

Here is the final tailored resume text (excluding header):

--------------------------
<FINAL_TAILORED_RESUME>
--------------------------

================
File: prompts/user/step1_user.md
================
--------------------------
<JOB_DESCRIPTION>
--------------------------

Please format your output as Markdown (bullet list).

================
File: prompts/user/step2_user.md
================
Here is my current resume:
---------------------------
<RESUME>
---------------------------

Here is the ranked list of keywords from the job description:
---------------------------
<RANKED_KEYWORDS>
---------------------------

Please output your planning‐table as a Markdown table.

================
File: prompts/user/step3_user.md
================
Here is my original resume:
---------------------------

<RESUME>
---------------------------

Here is the tailoring plan (from Step 2):
-----------------------------------------

<TAILORING_PLAN>
----------------

Please modify my resume according to the tailoring instructions.
Output ONLY the modified resume text (no explanations).

Format the modified resume itself in Markdown:

- Use headings like `## WORK EXPERIENCE`, `### Prodapt North America`, etc.
- Represent each bullet with `- ` or `* ` (not “●”).
- Preserve all section order (Work Experience → Education → Skills → Projects).
- Ensure each bullet contains **15–18 words** and begins with a strong action verb.
- Adapt wording to reflect any company-culture hints (e.g., “collaborative,” “fast-paced”).
- **If the Job Description specifies a new position title for “Prodapt North America,” replace the existing title “Software Engineer” entirely with that exact title (no parentheses).** Do not modify any other company’s titles.
- Match your phrasing and terminology to the language used in the job description.
- Never exceed line length or bullet count limits per section.
- Never output emoji, special characters, or marketing-like sentences.

**In short:** Return a complete Markdown-formatted resume with updated bullets (and if directed, the Prodapt title changed).

================
File: prompts/user/step4_user.md
================
--------------------------
<TAILORED_RESUME>
--------------------------

The above is my Step‐3 output. Please update only the “Skills” section
in the same Markdown format. That is:
- If the resume uses `## SKILLS` as a heading, keep it there.
- List each category under “Skills” as a bullet (e.g. `- Programming Languages:`).
- Do not remove any existing skill; only add missing ones.
- Preserve the rest of the resume exactly as Markdown.

Output the full, updated resume again in Markdown.

================
File: requirements.txt
================
python-dotenv
openai
python-docx



================================================================
End of Codebase
================================================================
